{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIZf5bazKMXJxiE/F/aUu4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahabbadihi/IT/blob/master/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8-6vawXzajP",
        "outputId": "781f3b16-eadc-4485-ff58-aec990bdf597"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['هارد خریدم سنگینی بک آپ اطلاعات عکس فیلم میکنمxD سرعت انتقال اطلاعاتش مناسبه کیفیت ساخت داره سال مشکل ایرادی نداشتهxD آداپتور اذیت هارد اونم توجه هارد دسکتاپ عجییبی نیستxD وزن هارد نمونه دسکتاپ خوبه']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# import sklearn\n",
        "# import pandas as pd\n",
        "# import requests\n",
        "# import io\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from spacy.lang.fa import stop_words\n",
        "# from string import punctuation, printable, digits\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "# from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# file_url = \"https://raw.githubusercontent.com/shahabbadihi/IT/master/2-p9vcb5bb.xlsx\"\n",
        "# response = requests.get(file_url)\n",
        "\n",
        "# dest = 'local-file.xlsx'\n",
        "\n",
        "# with open(dest, \"wb\") as filee:\n",
        "#   filee.write(response.content)\n",
        "\n",
        "# df = pd.read_excel(dest)\n",
        "\n",
        "# df.head(100)\n",
        "\n",
        "\n",
        "# df.nunique()\n",
        "\n",
        "\n",
        "# df.groupby('user_id').count().sort_values(by=['product_id'])\n",
        "\n",
        "# tdf = df\n",
        "# tdf['cart_id'] = 0\n",
        "# tdf.head(100)\n",
        "\n",
        "\n",
        "# user_list = np.array(tdf['user_id']).tolist()\n",
        "# user_list = list(set(user_list)) \n",
        "# print(user_list)\n",
        "\n",
        "comments = tdf[['product_id','user_id','comment']]\n",
        "comments.head(100)\n",
        "\n",
        "def load_stopwords():\n",
        "    f = open(\"/content/sample_data/fa_stop_words.txt\", \"r\", encoding='utf8')\n",
        "    stopwords = f.read()\n",
        "    stopwords = stopwords.split('\\n')\n",
        "    stopwords = set(stopwords)\n",
        "    custom_stop_words = {'آنكه','آيا','بدين','براين','بنابر','میشه','میکنه','باشه','سلام','میکشه','اونی',''}\n",
        "    stopwords = stopwords | stop_words.STOP_WORDS | custom_stop_words\n",
        "    # excluding space\n",
        "    stopwords = list(stopwords)[1:]\n",
        "    unwanted_num = {'خوش','بهتر','بد','خوب','نیستم','عالی','نیست','فوق','بهترین'} \n",
        "    stopwords = [ele for ele in stopwords if ele not in unwanted_num] \n",
        "    return stopwords\n",
        "# print (load_stopwords())\n",
        "\n",
        "# class Preprocess(BaseEstimator, TransformerMixin):\n",
        "#     def __init__(self, stop_words):\n",
        "#         self.stop_words = stop_words\n",
        "#     def fit(self, X, y=None):\n",
        "#         return self\n",
        "#     def transform(self, corpus):\n",
        "#         res = []\n",
        "#         for data in corpus:\n",
        "#             if not self.stop_words:\n",
        "#                 self.stop_words = set([])\n",
        "#             ## ensure working with string\n",
        "#             doc = str(data)\n",
        "#             # First remove punctuation form string\n",
        "#             PUNCT_DICT = {ord(punc): None for punc in punctuation+'،'}\n",
        "#             doc = doc.translate(PUNCT_DICT)\n",
        "#             # remove numbers\n",
        "#             doc = doc.translate({ord(k): None for k in digits})\n",
        "#             tokens = doc.split()\n",
        "#             tokens = [t for t in tokens if len(t) > 1]\n",
        "#             res.append(' '.join(w for w in tokens if w not in self.stop_words))\n",
        "#         return res\n",
        "\n",
        "\n",
        "\n",
        "Preprocess(load_stopwords()).transform([comments.comment[10]])\n",
        "\n",
        "# text_clf = Pipeline([\n",
        "#     ('prep', Preprocess(load_stopwords())),\n",
        "#     ('vect', CountVectorizer()),\n",
        "#     ('clf', MultinomialNB()),\n",
        "# ])\n",
        "\n",
        "# text_clf.predict(comments.comment)"
      ]
    }
  ]
}